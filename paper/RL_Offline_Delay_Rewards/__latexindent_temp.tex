\section{Introduction}

Offline reinforcement learning has received lots of attentions in recent years \cite{levine2020offline}, since it tries to learn policies with pre-collected data, which is especially attractive in real-world applications such as robotics, recommender systems etc. Among these tasks, an important but less studied setting is offline reinforcement learning with delay sparse rewards, which is quite common in some applications. For example in recommender systems,  the company often focus on long-term engagement of users such as daily active number or retention rate, instead of instant indicators like click or watch time. However, this kind of long-term indicators are sparse and delayed, since they can be only observed once a day after interactions between system and users in previous time. This kind of delayed and sparse signals are hard to be optimized directly, especially in more restricted and hard offline setting, so current recommender systems try to improve user engagement through short term response, which may not be optimal. In this paper, we focus on how to tackle offline reinforcement learning with delayed sparse rewards. 

However, most of existing algorithms designed for offline RL rely on dense rewards [XXX].  


