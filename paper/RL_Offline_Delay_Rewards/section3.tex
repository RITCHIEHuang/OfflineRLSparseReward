\section{Method}
In this section, we present a general framework for addressing sparse reward problems in offline setting. From a global perspective, the framework consists of two-stage learning tasks, in the first stage we perform specific transformation strategy on the offline datasets, and the second stage is a regular offline reinforcement learning task.

\textbf{Reward transformation.} Our key idea is to transmute the sparse delayed rewards into some dense instant rewards. Such transformation makes it possible to capture the meaningful patterns of original none-delayed rewards, 


\textbf{Offline policy optimization.}

